{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install scikit-learn\n",
    "!python --version"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "from keras import layers\n",
    "from keras.utils import timeseries_dataset_from_array, to_categorical\n",
    "import logging\n",
    "from enum import Enum\n",
    "import itertools\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#from utils import load_gesture_samples, GestureNames, split_data_between_participants, normalize_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Setup logger\n",
    "logging.basicConfig(level=logging.DEBUG,\n",
    "                    force = True)\n",
    "\n",
    "log = logging.getLogger(\"CSE3000\")\n",
    "log.setLevel(logging.DEBUG)\n",
    "\n",
    "np.random.seed(1000)\n",
    "tf.random.set_seed(1000)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Utility functions cell\n",
    "class Hand(Enum):\n",
    "    right = \"right_hand\"\n",
    "    left = \"left_hand\"\n",
    "\n",
    "\n",
    "class GestureException(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "class Gestures(Enum):\n",
    "    SWIPE_LEFT = 0, 'swipe_left'\n",
    "    SWIPE_RIGHT = 1, 'swipe_right'\n",
    "    SWIPE_UP = 2, 'swipe_up'\n",
    "    SWIPE_DOWN = 3, 'swipe_down'\n",
    "    ROT_CW = 4, 'clockwise'\n",
    "    ROT_CCW = 5, 'counter_clockwise'\n",
    "    TAP = 6, 'tap'\n",
    "    DOUBLE_TAP = 7, 'double_tap'\n",
    "    ZOOM_IN = 8, 'zoom_in'\n",
    "    ZOOM_OUT = 9, 'zoom_out'\n",
    "\n",
    "    def __new__(cls, value, name):\n",
    "        member = object.__new__(cls)\n",
    "        member._value_ = value\n",
    "        member.fullname = name\n",
    "        return member\n",
    "\n",
    "    def __int__(self):\n",
    "        return self.value\n",
    "\n",
    "    @staticmethod\n",
    "    def from_name(name: str):\n",
    "        try:\n",
    "            return next(g for g in Gestures if g.fullname == name)\n",
    "        except StopIteration:\n",
    "            raise GestureException(\"No gesture with name '%s' found...\" % name)\n",
    "\n",
    "\n",
    "class LoadGestureException(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "def load_gesture_samples(gesture: Gestures, hand: Hand = Hand.right, skip_old_data: bool = True):\n",
    "    result = []\n",
    "    base_path = f\"gestures_data/gestures/{gesture.fullname}/{hand.value}\"\n",
    "    log.debug(\"Loading gestures from base path: %s\" % base_path)\n",
    "    folder_items = os.listdir(base_path)\n",
    "\n",
    "    # Filter on the .pickle extension\n",
    "    filtered_ext = list(filter(lambda x: re.search(r'\\.pickle$', x) is not None, folder_items))\n",
    "\n",
    "    if len(filtered_ext) == 0:\n",
    "        raise LoadGestureException(\"No gestures found in folder: %s\" % base_path)\n",
    "\n",
    "    for item in filtered_ext:\n",
    "        r_match = re.match(r'candidate_(\\w+).pickle$', item)\n",
    "        if r_match is None:\n",
    "            raise LoadGestureException(\"Incorrectly formatted data file name: %s\" % item)\n",
    "\n",
    "        candidate_id = r_match.group(1)\n",
    "        with open(os.path.join(base_path, item), 'rb') as f:\n",
    "            while True:\n",
    "                try:\n",
    "                    data_contents = pickle.load(f)\n",
    "\n",
    "                    if isinstance(data_contents, dict):\n",
    "                        if 'target_gesture' in data_contents:\n",
    "                            # Data v3\n",
    "                            # print(data_contents)\n",
    "                            data_contents['gesture'] = Gestures.from_name(data_contents['target_gesture'])\n",
    "                            # data_contents['all_data'] = data_contents['data']\n",
    "                            # print(type(data_contents['data']))\n",
    "                            # data_contents['data'] = list(map(lambda x: x['data'], data_contents['data']))\n",
    "                            result.append(data_contents)\n",
    "                        else:\n",
    "                            # Data v2\n",
    "                            data_contents['gesture'] = Gestures.from_name(data_contents['gesture'])\n",
    "                            if not skip_old_data:\n",
    "                                result.append(data_contents)\n",
    "                    else:\n",
    "                        # Data loader v1\n",
    "                        data = {\n",
    "                            'data': data_contents,\n",
    "                            'gesture': gesture,\n",
    "                            'candidate': candidate_id\n",
    "                        }\n",
    "                        if not skip_old_data:\n",
    "                            result.append(data)\n",
    "                except EOFError:\n",
    "                    break\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def split_data_between_participants(data, ratio = 0.7):\n",
    "    lb_candidate = lambda x: x['candidate']\n",
    "\n",
    "    # For itertools.groupby to work we need to sort the data first\n",
    "    data.sort(key=lambda x: x['candidate'])\n",
    "\n",
    "    participants = set(map(lb_candidate, data))\n",
    "    amount_measurements = len(data)\n",
    "    amount_participants = len(participants)\n",
    "\n",
    "    log.debug(\"Participants: %s\" % participants)\n",
    "    log.debug(\"Got dataset for %d participants with %d measurements total\" % (amount_participants, amount_measurements))\n",
    "\n",
    "    amount_train = int(amount_measurements * 0.7)\n",
    "    amount_test = amount_measurements - amount_train\n",
    "    log.debug(\"Estimating %d measurements for training and %d measurements for test (ratio: %0.1f)\" % (amount_train, amount_test, ratio))\n",
    "\n",
    "    train_data = []\n",
    "    train_data_outcomes = []\n",
    "\n",
    "    test_data = []\n",
    "    test_data_outcomes = []\n",
    "\n",
    "    train_candidates = set()\n",
    "    test_candidates = set()\n",
    "\n",
    "    # Group the data per participant as that is the recommended method for training models\n",
    "    for participant, d in itertools.groupby(data, lb_candidate):\n",
    "        d_list = list(d)\n",
    "\n",
    "        for data_point in d_list:\n",
    "            if len(train_data) < amount_train:\n",
    "                train_candidates.add(participant)\n",
    "                train_data.append(data_point['data'])\n",
    "                train_data_outcomes.append(data_point['gesture'])\n",
    "            else:\n",
    "                test_candidates.add(participant)\n",
    "                test_data.append(data_point['data'])\n",
    "                test_data_outcomes.append(data_point['gesture'])\n",
    "                # test_data.extend([p['data'] for p in d_list])\n",
    "                # test_data_outcomes.extend([p['gesture'].value for p in d_list])\n",
    "\n",
    "    log.debug(\"Train candidates: %s\\tTest candidates: %s\" % (train_candidates, test_candidates))\n",
    "\n",
    "    return (np.array(train_data), np.array(train_data_outcomes)), (np.array(test_data), np.array(test_data_outcomes))\n",
    "\n",
    "def normalize_dataset(data):\n",
    "    \"\"\"Watch out this function might not normalize the data as expected, further research required\"\"\"\n",
    "    normalized = []\n",
    "    for graph in data:\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        reshaped = scaler.fit_transform(graph.reshape(-1, graph.shape[-1])).reshape(graph.shape)\n",
    "        normalized.append(reshaped)\n",
    "    return np.array(normalized)\n",
    "\n",
    "def normalize_test(data):\n",
    "    normalized = []\n",
    "    for measurement in data:\n",
    "        mean = measurement.mean()\n",
    "        std = measurement.std()\n",
    "        normalized_measurement = (measurement - mean) / std\n",
    "\n",
    "        normalized.append(normalized_measurement)\n",
    "    return normalized\n",
    "\n",
    "def plot_history(history):\n",
    "    loss = history.history['loss']\n",
    "    mae = history.history['mae']\n",
    "\n",
    "    epochs = range(1, len(loss) + 1)\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    axs[0].plot(epochs, loss, 'g.', label='Training Loss')\n",
    "    axs[0].set_xlabel('Epochs')\n",
    "    axs[0].set_ylabel('Loss')\n",
    "    axs[0].legend()\n",
    "\n",
    "    axs[1].plot(epochs, mae, 'g.', label='Training Accuracy')\n",
    "    axs[1].set_xlabel('Epochs')\n",
    "    axs[1].set_ylabel('MAE')\n",
    "    axs[1].legend()\n",
    "\n",
    "    fig.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "combined = []\n",
    "\n",
    "for g in Gestures:\n",
    "    samples = load_gesture_samples(g)\n",
    "    combined.extend(samples)\n",
    "\n",
    "# Deterministic shuffle\n",
    "random.Random(4).shuffle(combined)\n",
    "(x_train, y_train), (x_test, y_test) = split_data_between_participants(combined)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "x_train_normalized = normalize_dataset(x_train)\n",
    "x_test_normalized = normalize_dataset(x_test)\n",
    "y_train = to_categorical(y_train, len(Gestures))\n",
    "y_test = to_categorical(y_test, len(Gestures))\n",
    "\n",
    "# Hardcoded for now, so fix\n",
    "x_train_normalized = x_train_normalized.reshape((-1, 100, 3, 1))\n",
    "x_test_normalized = x_test_normalized.reshape((-1, 100, 3, 1))\n",
    "\n",
    "print(x_train_normalized.shape)\n",
    "print(x_test_normalized.shape)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Show all training data\n",
    "train_amount = len(x_train)\n",
    "fig, axs = plt.subplots(1, train_amount, figsize=(40, 4))\n",
    "for i in range(train_amount):\n",
    "    axs[i].plot(x_train[i])\n",
    "    axs[i].annotate(Gestures(y_train[i]).fullname, (0,0), (0, -40), xycoords='axes fraction', textcoords='offset points', va='top')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Show all test data\n",
    "test_amount = len(x_test)\n",
    "fig, axs = plt.subplots(1, test_amount, figsize=(20, 4))\n",
    "for i in range(test_amount):\n",
    "    axs[i].plot(x_test[i])\n",
    "    axs[i].annotate(f'Gesture: {Gestures(y_test[i]).fullname}', (0,0), (0, -40), xycoords='axes fraction', textcoords='offset points', va='top')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Show normalized test data\n",
    "test_amount = len(x_test_normalized)\n",
    "fig, axs = plt.subplots(1, test_amount, figsize=(20, 4))\n",
    "for i in range(test_amount):\n",
    "    axs[i].plot(x_test_normalized[i])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Implement data windowing\n",
    "# SLICES = 5\n",
    "#\n",
    "# single_source = np.array(train[0], dtype=np.uint16)\n",
    "# single_targets = np.full((single_source.shape[0], 1, 1), train_outcome[0])\n",
    "# print(single_source.shape)\n",
    "#\n",
    "# train_ds = timeseries_dataset_from_array(data=single_source, targets=single_targets, sequence_length=SLICES, sequence_stride=SLICES)\n",
    "#\n",
    "# for example_inputs, example_labels in train_ds.take(1):\n",
    "#     print(f'Inputs shape (batch, time, features): {example_inputs.shape}')\n",
    "#     print(f'Targets shape (batch, time, features): {example_labels.shape}')\n",
    "#\n",
    "#\n",
    "# single_validation_source = np.array(test[0], dtype=np.uint16)\n",
    "# single_validation_targets = np.full((single_validation_source.shape[0], 1, 1), test_outcome[0])\n",
    "#\n",
    "# validation_ds = keras.utils.timeseries_dataset_from_array(data=single_validation_source, targets=single_validation_targets, sequence_length=SLICES, sequence_stride=SLICES)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# train_ds = timeseries_dataset_from_array(data=x_train, targets=y_train, sequence_length=SLICES, sequence_stride=SLICES)\n",
    "# validation_ds = timeseries_dataset_from_array(data=x_test, targets=y_test, sequence_length=SLICES, sequence_stride=SLICES)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print(dir(train_ds))\n",
    "# print(train_ds.element_spec)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "USE_GPU = False\n",
    "device = \"/device:GPU:0\" if USE_GPU else \"/device:CPU:0\"\n",
    "\n",
    "with tf.device(device):\n",
    "    model_simple = keras.Sequential()\n",
    "\n",
    "    # model_lstm.add(layers.LSTM(units=128, input_shape=[1, 3]))\n",
    "    model_simple.add(layers.Input(shape=(100, 3, 1), name=\"sensor_image\"))\n",
    "    model_simple.add(layers.ZeroPadding2D(padding=(0, 2)))\n",
    "    model_simple.add(layers.Conv2D(8, kernel_size=(3, 3), strides=(1, 1), activation=\"relu\"))\n",
    "    model_simple.add(layers.Conv2D(16, kernel_size=(2, 2), strides=(1, 1), activation=\"relu\"))\n",
    "    model_simple.add(layers.Conv2D(16, kernel_size=(2, 2), activation=\"relu\"))\n",
    "    model_simple.add(layers.MaxPooling2D(pool_size=(3, 1)))\n",
    "    model_simple.add(layers.Conv2D(16, kernel_size=(5, 1), activation=\"relu\"))\n",
    "    model_simple.add(layers.Flatten())\n",
    "    model_simple.add(layers.Dropout(0.5))\n",
    "    model_simple.add(layers.Dense(len(Gestures), activation=\"softmax\", name=\"predictions\"))\n",
    "    model_simple.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc', 'mae'])\n",
    "    model_simple.summary()\n",
    "\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    simple_history = model_simple.fit(x_train_normalized, y_train, epochs=80, batch_size=2)\n",
    "\n",
    "    end = time.perf_counter()\n",
    "    print(\"Training took: %0.1f seconds\" % (end - start))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training times:\n",
    "| GPU   | CPU   |\n",
    "|-------|-------|\n",
    "| 18.5s | 12.3s |"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss = simple_history.history['loss']\n",
    "mae = simple_history.history['mae']\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "axs[0].plot(epochs, loss, 'g.', label='Training Loss')\n",
    "axs[0].set_xlabel('Epochs')\n",
    "axs[0].set_ylabel('Loss')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot(epochs, mae, 'g.', label='Training Accuracy')\n",
    "axs[1].set_xlabel('Epochs')\n",
    "axs[1].set_ylabel('MAE')\n",
    "axs[1].legend()\n",
    "\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "score = model_simple.evaluate(x_test_normalized, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(x_test_normalized.shape)\n",
    "\n",
    "for i in range(x_test_normalized.shape[0]):\n",
    "    test_sample = np.expand_dims(x_test_normalized[i], -4)\n",
    "    test_prediction = model_simple.predict(test_sample)\n",
    "    print(\"Prediction: %s, actual: %s\" % (Gestures(np.argmax(test_prediction)), Gestures(np.argmax(y_test[i]))))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Split data\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "USE_GPU = True\n",
    "device = \"/device:GPU:0\" if USE_GPU else \"/device:CPU:0\"\n",
    "\n",
    "with tf.device(device):\n",
    "    # https://towardsdatascience.com/time-series-classification-for-human-activity-recognition-with-lstms-using-tensorflow-2-and-keras-b816431afdff\n",
    "    model_lstm_stateless = keras.Sequential(name=\"LSTM_Stateless\")\n",
    "\n",
    "    # Subject to change as we split up the data in fragments:\n",
    "    # Stateless?\n",
    "    model_lstm_stateless.add(layers.Input(shape=(100, 3), name=\"sensor_image\"))\n",
    "    model_lstm_stateless.add(layers.LSTM(units=128, name=\"lstm\"))\n",
    "    # Stateful?\n",
    "    # model_lstm_stateless.add(layers.Input(batch_shape=(100, 3, 1), name=\"sensor_image\"))\n",
    "    # model_lstm_stateless.add(layers.LSTM(units=128, stateful=True, name=\"lstm\"))\n",
    "\n",
    "    # Extra\n",
    "    model_lstm_stateless.add(layers.Dropout(rate=0.5, name=\"dropout\"))\n",
    "    model_lstm_stateless.add(layers.Dense(units=128, activation='relu', name=\"dense_1\"))\n",
    "\n",
    "    # Output stage\n",
    "    model_lstm_stateless.add(layers.Dense(len(Gestures), activation=\"softmax\", name=\"predictions\"))\n",
    "    model_lstm_stateless.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc', 'mae'])\n",
    "    model_lstm_stateless.summary()\n",
    "\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    # Do we need to reset the states inbetween?\n",
    "    lstm_stateless_history = model_lstm_stateless.fit(x_train_normalized, y_train, epochs=1000, batch_size=1, shuffle=False)\n",
    "\n",
    "    end = time.perf_counter()\n",
    "    print(\"Training took: %0.1f seconds\" % (end - start))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_history(lstm_stateless_history)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "score = model_lstm_stateless.evaluate(x_test_normalized, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"================= Running on training data (Toy Example): =================\")\n",
    "for i in range(x_train_normalized.shape[0]):\n",
    "    test_sample = np.expand_dims(x_train_normalized[i], -4)\n",
    "    test_prediction = model_lstm_stateless.predict(test_sample)\n",
    "    print(\"Prediction: %s, actual: %s\" % (Gestures(np.argmax(test_prediction)), Gestures(np.argmax(y_train[i]))))\n",
    "\n",
    "print(\"\\n\\n================= Running on testing data: =================\")\n",
    "for i in range(x_test_normalized.shape[0]):\n",
    "    test_sample = np.expand_dims(x_test_normalized[i], -4)\n",
    "    test_prediction = model_lstm_stateless.predict(test_sample)\n",
    "    print(\"Prediction: %s, actual: %s\" % (Gestures(np.argmax(test_prediction)), Gestures(np.argmax(y_test[i]))))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "USE_GPU = True\n",
    "device = \"/device:GPU:0\" if USE_GPU else \"/device:CPU:0\"\n",
    "\n",
    "with tf.device(device):\n",
    "    # https://towardsdatascience.com/time-series-classification-for-human-activity-recognition-with-lstms-using-tensorflow-2-and-keras-b816431afdff\n",
    "    model_lstm_stateful = keras.Sequential(name=\"LSTM_Stateful\")\n",
    "\n",
    "    # Subject to change as we split up the data in fragments:\n",
    "    model_lstm_stateful.add(layers.Input(batch_shape=(100, 3, 1), name=\"sensor_image\"))\n",
    "    model_lstm_stateful.add(layers.LSTM(units=128, stateful=True, name=\"lstm\"))\n",
    "\n",
    "    # Extra\n",
    "    model_lstm_stateful.add(layers.Dropout(rate=0.5, name=\"dropout\"))\n",
    "    model_lstm_stateful.add(layers.Dense(units=128, activation='relu', name=\"dense_1\"))\n",
    "\n",
    "    # Output stage\n",
    "    model_lstm_stateful.add(layers.Dense(len(Gestures), activation=\"softmax\", name=\"predictions\"))\n",
    "    model_lstm_stateful.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc', 'mae'])\n",
    "    model_lstm_stateful.summary()\n",
    "\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    # Do we need to reset the states inbetween?\n",
    "    lstm_stateful_history = model_lstm_stateful.fit(x_train_normalized, y_train, epochs=1000, batch_size=1, shuffle=False)\n",
    "\n",
    "    end = time.perf_counter()\n",
    "    print(\"Training took: %0.1f seconds\" % (end - start))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_history(lstm_stateful_history)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
